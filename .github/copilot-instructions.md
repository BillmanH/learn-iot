# Copilot Instructions for Azure IoT Operations Learning Repository

## Important Constraints
⚠️ **Cannot Run Bash Scripts Directly**: The edge server (Linux) has no AI access. All bash scripts must be provided as code for the user to manually pull changes to the edge server and execute. Only PowerShell scripts can be run directly on the Windows development machine.

## Project Overview
This repository is focused on learning and implementing Azure IoT Operations (AIO) on edge devices using Kubernetes (K3s). It contains containerized IoT applications, deployment automation, and simulation tools for industrial IoT scenarios.

## Repository Structure

### Core Directories
- **`arc_build_linux/`** - Edge device setup scripts (runs ON the edge device)
  - `installer.sh` - Prepares edge device with K3s, kubectl, Helm, optional tools
  - `arc_enable.ps1` - Connects cluster to Azure Arc (runs from edge with PowerShell)
  - `arc_enable.ps1` now runs helm upgrade directly to enable custom-locations (no separate script needed)
  - Diagnostic scripts for K3s troubleshooting
- **`external_configuration/`** - Windows management scripts (runs FROM Windows machine)
  - `External-Configurator.ps1` - Deploys Azure infrastructure and IoT Operations via ARM templates
  - `grant_entra_id_roles.ps1` - Grants Entra ID roles, RBAC permissions, Key Vault policies
  - `Deploy-EdgeModules.ps1` - Deploys containerized modules to edge
- **`config/`** - Configuration files shared between scripts
  - `aio_config.json` - Main Azure configuration (subscription, resource group, cluster name)
  - `cluster_info.json` - Generated by installer.sh, consumed by External-Configurator.ps1
- **`arm_templates/`** - ARM templates for Azure resource deployment (IaC)
- **`modules/`** - IoT Operations applications (containerized apps for edge deployment)
  - `edgemqttsim/` - Industrial IoT simulator with configurable message generation
  - `sputnik/` - Simple MQTT test publisher
  - `hello-flask/` - REST API example
  - `demohistorian/` - Demo historian application
- **`operations/`** - Azure operations and integration configurations (YAML manifests)
- **`certs/`** - Certificate handling (base64 encoded)

### Key Applications
- **Sputnik** - MQTT publisher sending periodic "beep" messages for testing
- **Edge MQTT Simulator (edgemqttsim)** - Comprehensive industrial IoT simulator with modular message generation
  - Simulates factory equipment (CNC, 3D printers, welding, painting, testing rigs)
  - Configurable via YAML (`message_structure.yaml`) for message types, frequencies, and distributions
  - Supports business events (customer orders, dispatch notifications)
  - Intelligent topic routing and queue management
  - Built-in statistics tracking
- **Hello-Flask** - Simple REST API demonstrating containerized deployment
- **WASM Quality Filter** - WebAssembly module for real-time quality control filtering

## Technology Stack

### Primary Technologies
- **Azure IoT Operations** - Microsoft's edge computing platform
- **Kubernetes (K3s)** - Lightweight Kubernetes for edge devices
- **MQTT** - Message queuing protocol for IoT communication
- **Docker** - Containerization for edge applications
- **Python** - Primary development language with `uv` package manager
- **PowerShell** - Deployment automation scripts

### Azure Services
- **Azure IoT Hub** - Cloud IoT device management
- **Azure Arc** - Hybrid cloud management
- **Microsoft Fabric Real-Time Intelligence** - Analytics and visualization
- **Azure Container Registry** - Container image storage

## Development Patterns

### Deployment Approach
- **Prefer ARM Templates over Azure CLI** - ARM templates provide more stable and reliable deployments
  - Use ARM templates for: assets, asset endpoints, dataflows, and other Azure resources
  - ARM deployments are declarative and can be version-controlled
  - CLI commands are acceptable for ad-hoc queries and troubleshooting
  - Example: Use `az deployment group create --template-file` instead of `az iot ops dataflow create`

### Authentication
- **Preferred**: ServiceAccountToken (K8S-SAT) for in-cluster applications
- **Alternative**: X.509 certificates for external clients
- Applications should use MQTT v5 with enhanced authentication

### Container Deployment
- Applications follow standard Docker + Kubernetes pattern
- Each app includes: `Dockerfile`, `deployment.yaml`, `requirements.txt`, `README.md`
- Use unified deployment scripts: `Deploy-ToIoTEdge.ps1`, `Deploy-Local.ps1`, `Deploy-Check.ps1`

### MQTT Message Structure
- JSON format with consistent schema
- Required fields: `timestamp` (ISO 8601), `machine_id`, `station_id`, `status`
- Equipment-specific fields: `part_type`, `part_id`, `cycle_time`, `assembly_type`, `progress`
- Quality metrics: `good`, `scrap`, or `null` (for in-progress operations)
- Support for OEE calculations (Availability, Performance, Quality)
- Business event messages: order placement, dispatch/fulfillment

## Coding Guidelines

### File Naming Conventions
- Application folders: lowercase with hyphens (e.g., `hello-flask`)
- Python files: snake_case (e.g., `asset_creation.py`)
- Configuration files: kebab-case (e.g., `deployment.yaml`)
- Documentation: UPPERCASE for important docs (e.g., `README.md`)

### Python Development
- Use `uv` for dependency management instead of pip/poetry
- Include `pyproject.toml` for project configuration
- Follow containerized development pattern with Dockerfile
- Include health check endpoints for web applications

### PowerShell Scripts
- Modular deployment scripts with clear parameter documentation
- Support both Docker Hub and Azure Container Registry
- Include error handling and status checking
- Use consistent parameter naming across scripts
- **CRITICAL: Do NOT use special characters, emojis, or non-ASCII characters in PowerShell scripts** - they break script execution

## Common Operations

### Installation Workflow (Separation of Concerns)

The deployment follows a 4-step process across edge device and Windows management machine:

#### Step 1: Edge Device Setup (ON the Linux edge device)
```bash
# Clone the repo and configure
cd ~/learn-iot/arc_build_linux
cp ../config/aio_config.template.json ../config/aio_config.json
# Edit aio_config.json with your Azure settings

# Run installer to prepare K3s cluster
chmod +x installer.sh
./installer.sh

# Outputs: cluster_info.json with cluster details
```

#### Step 2: Azure Arc Connection (ON the Linux edge device with PowerShell)
```bash
# Install PowerShell if not present
sudo apt-get install -y powershell

# Run Arc enablement (uses Az.ConnectedKubernetes module)
# This script now automatically runs helm upgrade to enable custom-locations
pwsh ./arc_enable.ps1

# Verify custom-locations is enabled:
helm get values azure-arc -n azure-arc-release -o json | jq '.systemDefaultValues.customLocations'
```

**Note**: The Az.ConnectedKubernetes PowerShell module has a gap where `-CustomLocationsOid` only registers with ARM but doesn't update Helm. The `arc_enable.ps1` script now runs the required helm upgrade automatically.

#### Step 3: Azure Infrastructure & IoT Operations (FROM Windows machine)
```powershell
# Navigate to external configuration
cd external_configuration

# Deploy Azure infrastructure and IoT Operations
.\External-Configurator.ps1

# This deploys via ARM templates:
# - Resource Group, Managed Identity
# - Key Vault, Storage Account, Schema Registry
# - Azure IoT Operations (az iot ops init + create)
```

#### Step 4: Permissions Setup (FROM Windows machine)
```powershell
# Grant Entra ID roles and Key Vault access (uses current signed-in user)
.\grant_entra_id_roles.ps1

# Or grant to specific user - MUST use Object ID (GUID), NOT email
# Get your Object ID: az ad signed-in-user show --query id -o tsv
.\grant_entra_id_roles.ps1 -AddUser 12345678-1234-1234-1234-123456789abc
```

### Local Development
```bash
# Setup Python environment
uv sync

# Run application locally
.\Deploy-Local.ps1 -AppFolder "app-name" -Mode python

# For simulator: configure message generation
cd iotopps/edgemqttsim
# Edit message_structure.yaml to adjust message types and frequencies
```

### Edge Deployment
```powershell
# Deploy to remote IoT Operations cluster
.\Deploy-ToIoTEdge.ps1 -AppFolder "edgemqttsim" -RegistryName "username"

# Check deployment status
.\Deploy-Check.ps1 -AppFolder "edgemqttsim"
```

### MQTT Testing
```bash
# View MQTT messages
kubectl logs -n default -l app=mosquitto-sub -f

# Check Sputnik status
kubectl get pods -l app=sputnik
```

## Key Concepts to Understand

### Azure IoT Operations
- Edge-first platform running on Kubernetes
- Local MQTT broker for device communication
- Data flow processing and transformation
- Integration with Azure cloud services

### Industrial IoT Simulation (Edge MQTT Simulator)
- **Modular Architecture**: Separate `app.py` (MQTT client) and `messages.py` (message generation)
- **YAML Configuration**: All message types, frequencies, and parameters in `message_structure.yaml`
- **Equipment Types**: CNC machines, 3D printers, welding stations, painting booths, testing rigs
- **Business Events**: Customer orders and dispatch notifications
- **Topic Routing**: Intelligent routing to type-specific topics (e.g., `factory/cnc`, `factory/welding`)
- **State Management**: Realistic machine state tracking across cycles
- **Quality & OEE**: Configurable quality distributions supporting OEE calculations
- **Real-time Processing**: WASM modules for quality filtering
- **Analytics Integration**: Microsoft Fabric for visualization and insights

### Edge Computing Workflow
1. Devices/simulators → MQTT broker (edge)
2. Data flow processing → transformation/filtering
3. Cloud integration → Azure services
4. Analytics/visualization → Fabric Real-Time Intelligence

## Project-Specific Notes

### Certificate Management
- Certificates stored in base64 format in `certs/` directory
- Automated certificate setup scripts in `iotopps/`
- ServiceAccountToken preferred over X.509 for simplicity

### Troubleshooting Resources
- **K3s Diagnostics**: `k3s_troubleshoot.sh`, `k3s_advanced_diagnostics.sh`, `diagnose-orchestrator.sh`
- **Network Tools**: `test_network.sh`, `fix_k3s_ports.sh`, `fix_port_6443.sh`
- **Resource Discovery**: `find-namespace-resource.sh`, `check_discovery.sh`
- **IoT-Specific**: `IOT_TROUBLESHOOTING.md` in edgemqttsim, `fix-mqtt-connection.sh`
- **Installation Logs**: Detailed logging in `linuxAIO_*.log` files

### Quality Assurance
- **WASM Filtering**: Real-time quality control filtering at the edge
- **Configurable Quality**: Defect rates and quality distributions in `message_structure.yaml`
- **Quality Metrics**: Good/scrap tracking with part and assembly IDs
- **OEE Support**: Messages structured to calculate Availability, Performance, and Quality metrics
- **State Tracking**: Machine state management for accurate availability calculations

### Known Issues & Workarounds
- **Az.ConnectedKubernetes -CustomLocationsOid Gap**: The PowerShell module's `-CustomLocationsOid` parameter only registers the OID with Azure ARM but does NOT run `helm upgrade` to enable the feature in the cluster. The Azure CLI `az connectedk8s enable-features` does both steps. **Fix**: The `arc_enable.ps1` script now runs `helm upgrade` automatically to work around this gap.
- **Az.ConnectedKubernetes -WorkloadIdentityEnabled Gap**: Similar to custom-locations, setting `WorkloadIdentityEnabled = $true` in `New-AzConnectedKubernetes` only registers the feature with ARM but does NOT deploy the workload identity webhook pods to the cluster. Azure will show `workloadIdentityEnabled: true` but `kubectl get pods -n azure-arc | grep workload` returns nothing. **Fix**: The `arc_enable.ps1` script now runs `az connectedk8s update --enable-workload-identity` to deploy the webhook. Without this, Key Vault secret sync will fail with `AADSTS700211: No matching federated identity record found for presented assertion issuer 'https://kubernetes.default.svc.cluster.local'`.
- **Helm OCI Registry Issues**: Helm may fail to pull charts from OCI registries with "could not load config" errors. Workaround: Use `oras` to pull the chart, then upgrade with the local tgz file.
- **Device Registry Extension Bundled (AIO v1.2+)**: In older AIO versions, `microsoft.deviceregistry.assets` was a separate K8s extension. In AIO v1.2+, device registry functionality is **bundled into the `microsoft.iotoperations` extension**. Do NOT look for a separate `microsoft.deviceregistry.assets` extension - verify device registry is working by checking for CRDs instead:
  ```bash
  # Verify device registry CRDs exist (this confirms asset sync capability)
  kubectl get crd | grep -i deviceregistry
  # Expected: assets.deviceregistry.microsoft.com, assetendpointprofiles.deviceregistry.microsoft.com, etc.
  ```
- **Platform Extension Failures**: The `microsoft.iotoperations.platform` extension may fail during initial deployment due to CRD conflicts (e.g., `bundles.trust.cert-manager.io` already exists). **Fix**: Run `az iot ops upgrade --name <instance> -g <rg> -y` on the edge device to clean up failed extensions and update components.
- **Dual CRD System for Assets (AIO v1.2+)**: AIO v1.2 has TWO parallel APIs for assets that sync to DIFFERENT K8s CRDs:
  - **Old API** (CLI-created): `Microsoft.DeviceRegistry/assets` → syncs to `assets.deviceregistry.microsoft.com`
  - **New API** (Portal-created): `Microsoft.DeviceRegistry/namespaces/{ns}/assets` → syncs to `assets.namespaces.deviceregistry.microsoft.com`
  
  The shorthand `kubectl get assets -A` only shows OLD CRD assets. To see portal-created assets:
  ```bash
  # Old CRD (CLI-created assets)
  kubectl get assets.deviceregistry.microsoft.com -A
  
  # New CRD (Portal-created assets)
  kubectl get assets.namespaces.deviceregistry.microsoft.com -A
  
  # Similarly for devices (new namespace-nested CRD)
  kubectl get devices.namespaces.deviceregistry.microsoft.com -A
  ```
  
  When listing assets in Azure, use the namespace-nested resource type:
  ```powershell
  # Old API assets
  az resource list --resource-type "Microsoft.DeviceRegistry/assets" -o table
  
  # New API assets (portal-created)
  az resource list --resource-type "Microsoft.DeviceRegistry/namespaces/assets" -o table
  ```

### Troubleshooting Tips
When troubleshooting deployment issues, the most common cause is that **containers are still being created or deleted**. Be patient and verify the cluster state before proceeding:

```bash
# Check if all pods are running (no Pending, ContainerCreating, or Terminating)
kubectl get pods -A | grep -v "Running\|Completed"

# Watch pods until stable (Ctrl+C to exit)
watch -n 2 'kubectl get pods -A | grep -v "Running\|Completed"'

# Check for stuck namespaces
kubectl get ns | grep Terminating

# Verify Azure Arc pods are healthy (should be 12+ pods Running)
kubectl get pods -n azure-arc

# Check Azure IoT Operations pods
kubectl get pods -n azure-iot-operations
```

**Wait until all pods are in Running/Completed state before running the next deployment step.**

## Avoid These Patterns
- Don't use plain MQTT without authentication
- Don't deploy without containerization
- Don't hardcode certificates or connection strings
- Don't ignore health check implementations
- Don't use deprecated MQTT v3.1.1 (prefer v5)

## When Helping with Code
1. Follow the established containerized deployment pattern
2. Use ServiceAccountToken authentication for in-cluster apps
3. Include proper error handling and logging
4. Add health check endpoints for web services
5. Follow the modular PowerShell deployment script pattern
6. Include comprehensive README documentation
7. Use `uv` for Python dependency management
8. Consider real-time processing requirements for IoT data
8. Consider real-time processing requirements for IoT data